#!/usr/bin/env python3
"""
ä¸¦åˆ—AIãƒ¬ãƒ“ãƒ¥ãƒ¼ã®çµ±åˆå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

è¤‡æ•°ã®AIãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®å‡ºåŠ›ã‚’çµ±åˆã—ã€é‡è¤‡æ’é™¤ãƒ»å„ªå…ˆåº¦ä»˜ã‘ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
"""

import argparse
import json
import re
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple


# ========================================
# å®šæ•°å®šç¾©
# ========================================

FILE_LINE_PATTERN = r'\*\*\[([^\]]+?):(\d+)\]\*\*\s+([^\n]+)'
MAX_FILE_PATH_LENGTH = 500
MAX_LINE_LENGTH = 500

CATEGORY_PATTERNS = {
    'security': [
        'injection', 'xss', 'csrf', 'authentication',
        'authorization', 'crypto', 'ssl', 'tls', 'password',
        'secret', 'token', 'auth', 'vulnerable', 'exploit',
        'vulnerability', 'owasp', 'cwe', 'sql', 'command',
        'broken_access', 'idor', 'cryptographic', 'deserialization',
        'misconfiguration', 'ssrf', 'xxe'
    ],
    'performance': [
        'n+1', 'slow', 'inefficient', 'memory', 'leak',
        'optimization', 'cache', 'bottleneck', 'timeout',
        'deadlock', 'latency', 'throughput', 'query'
    ],
    'code_quality': [
        'readability', 'maintainability', 'dry', 'duplication',
        'naming', 'complexity', 'refactor', 'style',
        'convention', 'format', 'lint', 'clean'
    ],
    'architecture': [
        'pattern', 'coupling', 'cohesion', 'solid',
        'design', 'principle', 'abstraction', 'layer',
        'module', 'component', 'dependency', 'circular'
    ],
    'test': [
        'coverage', 'edge case', 'test', 'mock',
        'unit', 'integration', 'e2e', 'pytest', 'assert'
    ],
    'github': [
        'ci/cd', 'github', 'action', 'workflow',
        'deploy', 'release', 'branch', 'merge', 'pr'
    ]
}

CRITICAL_SECURITY_PATTERNS = [
    # A01: Broken Access Control
    'broken_access_control', 'idor', 'insecure_direct_object_reference',
    'authorization_bypass', 'authentication_bypass',
    # A02: Cryptographic Failures
    'sensitive_data_exposure', 'cryptographic_failure', 'encryption',
    'unencrypted', 'plaintext', 'hardcoded_secret', 'hardcoded_password',
    # A03: Injection
    'sql_injection', 'nosql_injection', 'command_injection',
    'code_injection', 'ldap_injection', 'xpath_injection',
    'os_command_injection', 'template_injection',
    # A04: Insecure Design
    'insecure_design', 'insecure_deserialization', 'deserialization',
    # A05: Security Misconfiguration
    'security_misconfiguration', 'misconfiguration',
    # A07: Auth Failures
    'weak_authentication', 'session_fixation', 'credential_exposure',
    # A10: SSRF
    'ssrf', 'server_side_request_forgery',
    # ãã®ä»–
    'xss', 'cross_site_scripting', 'reflected_xss', 'stored_xss', 'dom_xss',
    'csrf', 'cross_site_request_forgery',
    'xxe', 'xml_external_entity', 'xml_bomb',
    'path_traversal', 'directory_traversal', 'local_file_inclusion',
    'race_condition',
]


# ========================================
# Finding ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
# ========================================

class Finding:
    """ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®æŒ‡æ‘˜ã‚’è¡¨ã™ã‚¯ãƒ©ã‚¹"""

    def __init__(self, file: str, line: int, category: str,
                 priority: str, description: str, reviewer: str,
                 detailed_comment: str = ""):
        self.file = file[:MAX_FILE_PATH_LENGTH]
        self.line = line
        self.category = category
        self.priority = priority
        self.description = description[:MAX_LINE_LENGTH]
        self.reviewer = reviewer
        self.detailed_comment = detailed_comment
        self.reviewers = [reviewer]
        self.reviewer_comments = {reviewer: detailed_comment}

    def to_dict(self) -> Dict:
        return {
            'file': self.file,
            'line': self.line,
            'category': self.category,
            'priority': self.priority,
            'description': self.description,
            'reviewers': self.reviewers,
            'reviewer_count': len(self.reviewers),
        }

    def key(self) -> Tuple:
        """é‡è¤‡æ’é™¤ç”¨ã®ã‚­ãƒ¼"""
        return (self.file, self.line, self.category)


# ========================================
# ãƒ‘ãƒ¼ã‚¹å‡¦ç†
# ========================================

def classify_category(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’è‡ªå‹•åˆ†é¡"""
    text_lower = text.lower()

    for category, keywords in CATEGORY_PATTERNS.items():
        for keyword in keywords:
            if keyword in text_lower:
                return category

    return 'code_quality'


def extract_priority(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å„ªå…ˆåº¦ã‚’æŠ½å‡º"""
    text_lower = text.lower()

    if any(marker in text_lower for marker in ['ğŸ”´', 'critical', 'ç·Šæ€¥']):
        return 'Critical'
    elif any(marker in text_lower for marker in ['ğŸŸ¡', 'ğŸŸ ', 'important', 'high', 'é‡è¦']):
        return 'High'
    elif any(marker in text_lower for marker in ['ğŸŸ¢', 'medium', 'ä¸­', 'suggestion']):
        return 'Medium'
    else:
        return 'Low'


def parse_reviewer_output(reviewer_name: str, output_text: str) -> List[Finding]:
    """ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®å‡ºåŠ›ã‚’ãƒ‘ãƒ¼ã‚¹"""
    findings = []

    if not output_text or 'error' in output_text.lower():
        return findings

    matches = re.finditer(FILE_LINE_PATTERN, output_text)

    for match in matches:
        file_path = match.group(1)
        line_num = int(match.group(2))
        description = match.group(3)

        category = classify_category(description)
        priority = extract_priority(description)

        finding = Finding(
            file=file_path,
            line=line_num,
            category=category,
            priority=priority,
            description=description,
            reviewer=reviewer_name,
            detailed_comment=description
        )

        findings.append(finding)

    return findings


# ========================================
# é‡è¤‡æ’é™¤å‡¦ç†
# ========================================

def deduplicate_exact_match(all_findings: List[Finding]) -> List[Finding]:
    """å®Œå…¨ä¸€è‡´ã«ã‚ˆã‚‹é‡è¤‡æ’é™¤"""
    dedup_map = {}

    for finding in all_findings:
        key = finding.key()

        if key not in dedup_map:
            dedup_map[key] = finding
        else:
            # æ—¢å­˜ã®findingã«ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼æƒ…å ±ã‚’è¿½åŠ 
            existing = dedup_map[key]
            if finding.reviewer not in existing.reviewers:
                existing.reviewers.append(finding.reviewer)
                existing.reviewer_comments[finding.reviewer] = finding.detailed_comment

    return list(dedup_map.values())


def deduplicate_fuzzy_match(exact_deduplicated: List[Finding]) -> List[Finding]:
    """Fuzzy Matchingã«ã‚ˆã‚‹é‡è¤‡æ’é™¤ï¼ˆO(n)å®Ÿè£…ï¼‰"""
    # ç©ºé–“ç´¢å¼•ã®æ§‹ç¯‰
    spatial_index = defaultdict(lambda: defaultdict(list))

    for finding in exact_deduplicated:
        key = (finding.file, finding.category)
        spatial_index[key][finding.line].append(finding)

    # å‡¦ç†æ¸ˆã¿ã®findingã‚’è¿½è·¡
    processed = set()
    merged = []

    for i, finding in enumerate(exact_deduplicated):
        if i in processed:
            continue

        file_path = finding.file
        line = finding.line
        category = finding.category
        key = (file_path, category)

        # Â±2è¡Œä»¥å†…ã®findingã‚’æ¤œç´¢
        related = []
        related_indices = set()

        for offset in range(-2, 3):
            check_line = line + offset
            if check_line in spatial_index[key]:
                for idx, related_finding in enumerate(spatial_index[key][check_line]):
                    idx_in_main = exact_deduplicated.index(related_finding)
                    related.append(related_finding)
                    related_indices.add(idx_in_main)

        # Fuzzy Matchçµæœã‚’çµ±åˆ
        if len(related) > 1:
            merged_finding = related[0]
            for r in related[1:]:
                for reviewer in r.reviewers:
                    if reviewer not in merged_finding.reviewers:
                        merged_finding.reviewers.append(reviewer)
                        merged_finding.reviewer_comments[reviewer] = r.reviewer_comments[reviewer]

            merged.append(merged_finding)
            processed.update(related_indices)
        else:
            merged.append(finding)
            processed.add(i)

    return merged


# ========================================
# å„ªå…ˆåº¦ä»˜ã‘å‡¦ç†
# ========================================

def calculate_priority_from_count(reviewer_count: int) -> str:
    """ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼æ•°ã‹ã‚‰å„ªå…ˆåº¦ã‚’è¨ˆç®—"""
    if reviewer_count >= 4:
        return 'Critical'
    elif reviewer_count >= 3:
        return 'High'
    elif reviewer_count >= 2:
        return 'Medium'
    else:
        return 'Low'


def escalate_security_priority(finding: Finding) -> Finding:
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è„†å¼±æ€§ã®å„ªå…ˆåº¦ã‚’è‡ªå‹•æ˜‡æ ¼"""
    if finding.category != 'security':
        return finding

    description_lower = finding.description.lower()

    for pattern in CRITICAL_SECURITY_PATTERNS:
        if pattern in description_lower:
            finding.priority = 'Critical'
            return finding

    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œã¯æœ€ä½ã§ã‚‚Highã«
    if finding.priority in ['Low', 'Medium']:
        finding.priority = 'High'

    return finding


def apply_priority_calculation(findings: List[Finding]) -> List[Finding]:
    """å„ªå…ˆåº¦ã‚’å†è¨ˆç®—"""
    for finding in findings:
        # ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼æ•°ã‹ã‚‰åŸºæœ¬å„ªå…ˆåº¦ã‚’è¨ˆç®—
        count_based_priority = calculate_priority_from_count(len(finding.reviewers))

        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è„†å¼±æ€§ã¯æ˜‡æ ¼ã®å¯¾è±¡
        finding.priority = count_based_priority
        finding = escalate_security_priority(finding)

    return findings


# ========================================
# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
# ========================================

def generate_report(findings: List[Finding], failed_reviewers: List[str] = None) -> str:
    """çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    if failed_reviewers is None:
        failed_reviewers = []

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡
    by_priority = defaultdict(list)
    by_category = defaultdict(list)

    for finding in findings:
        by_priority[finding.priority].append(finding)
        by_category[finding.category].append(finding)

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = []
    report.append("# ä¸¦åˆ—AIãƒ¬ãƒ“ãƒ¥ãƒ¼çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ\n")

    # ã‚µãƒãƒªãƒ¼
    report.append("## ğŸ“Š ã‚µãƒãƒªãƒ¼\n")
    total = len(findings)
    critical = len(by_priority.get('Critical', []))
    high = len(by_priority.get('High', []))
    medium = len(by_priority.get('Medium', []))
    low = len(by_priority.get('Low', []))

    report.append(f"- **æ¤œå‡ºå•é¡Œç·æ•°**: {total}ä»¶\n")
    report.append(f"  - ğŸ”´ Critical: {critical}ä»¶\n")
    report.append(f"  - ğŸŸ¡ High: {high}ä»¶\n")
    report.append(f"  - ğŸŸ¢ Medium: {medium}ä»¶\n")
    report.append(f"  - â„¹ï¸ Low: {low}ä»¶\n")

    if failed_reviewers:
        report.append(f"\nâš ï¸ **å¤±æ•—ã—ãŸãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼**: {', '.join(failed_reviewers)}\n")

    # å„ªå…ˆåº¦åˆ¥ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    for priority_level in ['Critical', 'High', 'Medium', 'Low']:
        if priority_level not in by_priority:
            continue

        priority_map = {'Critical': 'ğŸ”´', 'High': 'ğŸŸ¡', 'Medium': 'ğŸŸ¢', 'Low': 'â„¹ï¸'}
        report.append(f"\n## {priority_map[priority_level]} {priority_level}\n")

        for finding in by_priority[priority_level]:
            report.append(f"### [{finding.file}:{finding.line}] {finding.description}\n")
            report.append(f"**ã‚«ãƒ†ã‚´ãƒª**: {finding.category}\n")
            report.append(f"**æŒ‡æ‘˜ã—ãŸãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼**: {', '.join(finding.reviewers)} ({len(finding.reviewers)}/4)\n")
            report.append("")

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
    if by_category:
        report.append("\n## ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ\n")

        for category in sorted(by_category.keys()):
            count = len(by_category[category])
            report.append(f"### {category.replace('_', ' ').title()} ({count}ä»¶)\n")
            report.append("")

    # çµ±è¨ˆæƒ…å ±
    report.append("\n## çµ±è¨ˆæƒ…å ±\n\n")
    report.append(f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().isoformat()}\n")

    return "".join(report)


# ========================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ========================================

def main():
    parser = argparse.ArgumentParser(description='ä¸¦åˆ—AIãƒ¬ãƒ“ãƒ¥ãƒ¼ã®çµ±åˆå‡¦ç†')
    parser.add_argument('--codex', help='Codexã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--coderabbit', help='CodeRabbitã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--copilot', help='Copilotã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--gemini', help='Geminiã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--output', help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹', default='integrated-report.md')

    args = parser.parse_args()

    # å„ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®å‡ºåŠ›ã‚’èª­ã¿è¾¼ã¿
    all_findings = []
    failed_reviewers = []
    reviewers = {'codex': args.codex, 'coderabbit': args.coderabbit,
                 'copilot': args.copilot, 'gemini': args.gemini}

    for reviewer_name, file_path in reviewers.items():
        if file_path and Path(file_path).exists():
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                findings = parse_reviewer_output(reviewer_name, content)
                all_findings.extend(findings)
            except Exception as e:
                print(f"Error reading {reviewer_name}: {e}")
                failed_reviewers.append(reviewer_name)
        else:
            failed_reviewers.append(reviewer_name)

    # é‡è¤‡æ’é™¤å‡¦ç†
    exact_deduplicated = deduplicate_exact_match(all_findings)
    deduplicated = deduplicate_fuzzy_match(exact_deduplicated)

    # å„ªå…ˆåº¦ä»˜ã‘å‡¦ç†
    findings_with_priority = apply_priority_calculation(deduplicated)

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = generate_report(findings_with_priority, failed_reviewers)

    # å‡ºåŠ›
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"âœ… çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {output_path}")
    print(f"æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ: {len(findings_with_priority)}ä»¶")


if __name__ == '__main__':
    main()
